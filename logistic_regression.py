# -*- coding: utf-8 -*-
"""Logistic Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YuBJciiVxj36jPnFds4a993hvVetQ4__
"""

# Import basic libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

url = '/content/drive/MyDrive/tested.csv'
df = pd.read_csv(url)
df.head()

df.shape

df.info()

# checking nulls
df.isnull().sum()

df.duplicated().sum()

#Filling Fare

#Filling nulls with the mean fare of their Pclass

df[df.Fare.isnull()]

mean_c3 = df.Fare[df.Pclass == 3].mean()
df['Fare'].fillna(value=mean_c3, inplace=True)

mean_age_miss = df[df["Name"].str.contains('Miss.', na=False)]['Age'].mean().round()
mean_age_mrs = df[df["Name"].str.contains('Mrs.', na=False)]['Age'].mean().round()
mean_age_mr = df[df["Name"].str.contains('Mr.', na=False)]['Age'].mean().round()
mean_age_master = df[df["Name"].str.contains('Master.', na=False)]['Age'].mean().round()

print("The mean age of Miss title: ",mean_age_miss)
print("The mean age of Mrs title: ",mean_age_mrs)
print("The mean age of Mr title: ",mean_age_mr)
print("The mean age of Master: ",mean_age_master)

def filling_age(name_age):
    name = name_age[0]
    age = name_age[1]
    if pd.isnull(age):
        if 'Mr.' in name:
            return mean_age_mr
        if 'Mrs.' in name:
            return mean_age_mrs
        if 'Miss.' in name:
            return mean_age_miss
        if 'Master.' in name:
            return mean_age_master
        if 'Dr.' in name:
            return mean_age_master
        if 'Ms.' in name:
            return mean_age_miss
    else:
        return age
df['Age'] = df[['Name', 'Age']].apply(filling_age,axis=1)

df.drop(['Cabin'], axis=1, inplace=True)

df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)

df.nunique()

df.dtypes

# put the categorical Featurs in dataframe
df_cat = df[['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'Survived']]
df_cat.head()

# make the dataframe attributes realted to each other
categories = {"female": 1, "male": 0}
df['Sex']= df['Sex'].map(categories)

new_df = pd.concat([df, pd.get_dummies(df['Embarked'],drop_first=True)], axis=1)

new_df.drop(['Embarked'], axis=1, inplace=True)

new_df.head()

new_df.info()

# Exploring the target variable
sns.countplot(x='Survived', data=new_df)
plt.title('Survival Count')
plt.show()

# Exploring categorical variables
sns.countplot(x='Sex', hue='Survived', data=new_df)
plt.title('Survival Count by Gender')
plt.show()

sns.countplot(x='Pclass', hue='Survived', data=new_df)
plt.title('Survival Count by Passenger Class')
plt.show()

corr = new_df.corr()
corr

plt.figure(figsize=(15,10))
sns.heatmap(corr, cbar=True, fmt='.2f', annot=True, annot_kws={'size':15}, cmap='Reds')

new_df.drop(['Sex'], axis=1, inplace=True)

new_df.hist(bins=50, figsize=(10,7))
plt.show()

# Clearer histogram plots for the Age and  Fare columns
sns.histplot(x='Age', data=new_df, kde=True)
plt.title('Age Distribution')
plt.show()

sns.histplot(x='Fare', data=new_df, kde=True)
plt.title('Fare Distribution')
plt.show()

# Calculate IQR
q1 = new_df['Fare'].quantile(0.25)
q3 = new_df['Fare'].quantile(0.75)
iqr = q3 - q1

# Define range
lower_range = q1 - 1.5 * iqr
upper_range = q3 + 1.5 * iqr

# Identify outliers
outlier_indices = new_df[(new_df['Fare'] < lower_range) | (new_df['Fare'] > upper_range)].index

# Remove outliers
cleaned_df = new_df.drop(outlier_indices)

## importing modelling libraries
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

X = cleaned_df.drop(['Survived'], axis=1)
y = cleaned_df['Survived']

## Data normalization
from sklearn.preprocessing import MinMaxScaler
mns = MinMaxScaler()
X = pd.DataFrame(mns.fit_transform(X), columns=X.columns)

"""Training models

"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30, random_state=42)

#RANDOM FOREST
rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=2)
rf.fit(X_train, y_train)
print('Training score: ', round(rf.score(X_train, y_train),3))
print('Testing score: ', round(rf.score(X_test, y_test),3))

#LOGISTIC REGRESSION
logreg = LogisticRegression()
logreg.fit(X_train, y_train)
print('Training score: ', round(logreg.score(X_train, y_train),3))
print('Testing score: ', round(logreg.score(X_test, y_test),3))

#SVM
svm = SVC()
svm.fit(X_train, y_train)
print('Training score: ', round(svm.score(X_train, y_train),3))
print('Testing score: ', round(svm.score(X_test, y_test),3))

knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
print('Training score: ', round(knn.score(X_train, y_train),3))
print('Testing score: ', round(knn.score(X_test, y_test),3))

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
y_pred = logreg.predict(X_test)

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

#Confusion Matrix
from sklearn.metrics import confusion_matrix
cma=confusion_matrix(y_test, y_pred)
sns.heatmap(cma,annot=True)

y_log = logreg.predict(X_test)

y_log

matrix_log=confusion_matrix(y_test,y_log)
print(matrix_log,'\n')
print(ConfusionMatrixDisplay.from_predictions(y_test,y_log))
accuracy_log=accuracy_score(y_test,y_log)
print(accuracy_naive,'\n')
report_log=classification_report(y_test,y_log)
print(report_log)

from sklearn.linear_model import LogisticRegression
lrmod = LogisticRegression()
lrmod.fit(X_train, y_train)
from sklearn.metrics import accuracy_score
y_predict = lrmod.predict(X_test)
accuracy_score(y_test, y_predict)

#SVM model alg
from sklearn.svm import SVC
model_SVC=SVC()
model_SVC.fit(X_train,y_train)

y_SVC=model_SVC.predict(X_test)
y_SVC

from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,ConfusionMatrixDisplay

matrix_SVC=confusion_matrix(y_test,y_SVC)
print(matrix_SVC,'\n')
print(ConfusionMatrixDisplay.from_predictions(y_test,y_SVC))
accuracy_SVC=accuracy_score(y_test,y_SVC)
print(accuracy_SVC,'\n')
report_SVC=classification_report(y_test,y_SVC)
print(report_SVC)

y_dec=model_dec.predict(X_test)
y_dec

#Decision Tree model alg
from sklearn.tree import DecisionTreeClassifier
model_dec=DecisionTreeClassifier(criterion='entropy')
model_dec.fit(X_train,y_train)

matrix_dec=confusion_matrix(y_test,y_dec)
print(matrix_dec,'\n')
print(ConfusionMatrixDisplay.from_predictions(y_test,y_dec))
accuracy_dec=accuracy_score(y_test,y_dec)
print(accuracy_dec,'\n')
report_dec=classification_report(y_test,y_dec)
print(report_dec)

#Randomforest classifier
from sklearn.ensemble import RandomForestClassifier
model_ran=RandomForestClassifier(n_estimators=25,criterion='entropy')
model_ran.fit(X_train,y_train)

y_ran=model_ran.predict(X_test)
y_ran

matrix_dec=confusion_matrix(y_test,y_ran)
print(matrix_dec,'\n')
print(ConfusionMatrixDisplay.from_predictions(y_test,y_ran))
accuracy_ran=accuracy_score(y_test,y_ran)
print(accuracy_ran,'\n')
report_ran=classification_report(y_test,y_ran)
print(report_ran)

#KNN model alg
from sklearn.neighbors import KNeighborsClassifier
model_KNN=KNeighborsClassifier(n_neighbors=5)
model_KNN.fit(X_train,y_train)

y_KNN=model_KNN.predict(X_test)
y_KNN

from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,ConfusionMatrixDisplay
matrix_KNN=confusion_matrix(y_test,y_KNN)
print(matrix_KNN,'\n')
print(ConfusionMatrixDisplay.from_predictions(y_test,y_KNN))
accuracy_KNN=accuracy_score(y_test,y_KNN)
print(accuracy_KNN,'\n')
report_KNN=classification_report(y_test,y_KNN)
print(report_KNN)

#naive bayes model alg
from sklearn.naive_bayes import MultinomialNB
model_naive=MultinomialNB()
model_naive.fit(X_train,y_train)

y_naive=model_naive.predict(X_test)
y_naive

matrix_naive=confusion_matrix(y_test,y_naive)
print(matrix_naive,'\n')
print(ConfusionMatrixDisplay.from_predictions(y_test,y_naive))
accuracy_naive=accuracy_score(y_test,y_naive)
print(accuracy_naive,'\n')
report_naive=classification_report(y_test,y_naive)
print(report_naive)

alg=['KNN','Naive Bayes','SVM','Decision Tree','Random Forest','Logistic Regression']
acc=[accuracy_KNN,accuracy_naive,accuracy_log,accuracy_SVC,accuracy_dec,accuracy_ran]
Accuracy_Scores=pd.DataFrame({'Algorithms':alg, 'Accuracy': acc})
Accuracy_Scores['Accuracy']=Accuracy_Scores['Accuracy']*100
Accuracy_Scores

#sorting models based on their accuracy score
Accuracy_Scores.sort_values(by='Accuracy',ascending=False)

# Specify the figure size
plt.figure(figsize=(10, 6))

# Create the barplot
ax = sns.barplot(x='Algorithms', y='Accuracy',
                 palette='muted', data=Accuracy_Scores.sort_values(by='Accuracy',ascending=False),
                 errwidth=0)

# Add labels to the bars
for i in ax.containers:
    ax.bar_label(i)

# Display the plot
plt.show()